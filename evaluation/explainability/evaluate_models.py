"""
This performs a analysis of caption quality and semantic similarity
for different VLMs (BLIP, Florence2, BLIP-LoRA). It evaluates
captions generated for both real and synthetic images across various domains (IO and NIO products).

The script is structured to perform two main tasks:
1.  It computes basic statistics for the captions generated by each system,
    such as the average caption length and the average number of predefined defect-related keywords.

2.  It compares the captions generated for real images against those for synthetic images on a
    one-to-one basis. This comparison uses a suite of standard and advanced NLP metrics to
    quantify how semantically similar the real vs. synthetic captions are.

The script loads caption data from a predefined directory structure, calculates all
metrics, and saves the results into two separate CSV files in the 'outputs/reports/'
directory:
-   `summary.csv`: Contains the summary statistics (avg length, avg keywords).
-   `pairwise_similarity.csv`: Contains the semantic similarity scores (BLEU, ROUGE,
    METEOR, BERTScore, etc.) between real and synthetic image captions.
"""

import os
import csv
import numpy as np
from collections import defaultdict
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from nltk.translate.meteor_score import meteor_score
from rouge_score import rouge_scorer
from sentence_transformers import SentenceTransformer, util
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import pairwise_distances
from bert_score import score as bert_score
from transformers import CLIPProcessor, CLIPModel
import torch

# config
base_dirs = {
    "BLIP": "../../data/BLIP",
    "Florence2": "../../data/Florence2",
    "BLIP-LoRA": "../../data/BLIP-LoRA"
}

subfolders = [
    "real_io",
    "real_nio",
    "syn_io",
    "syn_nio"
]

# pre-defined words for showing defects in bakery products
defect_keywords = ["screw", "plastic", "malformed", "broken", "deformed", "hole", "foreign", "misshaped", "foil"]

sts_model = SentenceTransformer("all-MiniLM-L6-v2")
rouge = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=True)
smooth_fn = SmoothingFunction().method1

clip_device = "cuda" if torch.cuda.is_available() else "cpu"
try:
    clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(clip_device)
    clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    do_clipscore = True
except Exception:
    clip_model = None
    clip_processor = None
    do_clipscore = False

os.makedirs("../../outputs/reports", exist_ok=True)
summary_csv = "../../outputs/reports/summary.csv"
similarity_csv = "../../outputs/reports/pairwise_similarity.csv"


def load_captions(folder_path):
    captions = {}
    for fname in os.listdir(folder_path):
        if not fname.lower().endswith(".txt"):
            continue
        img_name = fname.replace(".txt", ".png")
        with open(os.path.join(folder_path, fname), "r") as f:
            cap = f.read().strip().lower()
        captions[img_name] = cap
    return captions


def keyword_score(caption: str) -> int:
    return sum(1 for kw in defect_keywords if kw in caption.split())


def get_tfidf_similarity(caps_a, caps_b):
    vec = TfidfVectorizer().fit(caps_a + caps_b)
    tfidf_a = vec.transform(caps_a)
    tfidf_b = vec.transform(caps_b)
    sim_matrix = 1 - pairwise_distances(tfidf_a, tfidf_b, metric="cosine")
    return float(np.mean(np.diag(sim_matrix)))


def get_sts_similarity(caps_a, caps_b):
    emb_a = sts_model.encode(caps_a, convert_to_tensor=True)
    emb_b = sts_model.encode(caps_b, convert_to_tensor=True)
    sims = util.cos_sim(emb_a, emb_b).diagonal()
    return float(sims.mean().cpu().numpy())


def compute_bleu(reference_caps, generated_caps):
    total_bleu = 0.0
    count = 0

    for ref, hyp in zip(reference_caps, generated_caps):
        ref_tokens = ref.split()
        hyp_tokens = hyp.split()

        if len(ref_tokens) == 0 or len(hyp_tokens) == 0:
            continue

        bleu_sc = sentence_bleu(
            [ref_tokens],
            hyp_tokens,
            weights=(0.25, 0.25, 0.25, 0.25),
            smoothing_function=smooth_fn
        )
        total_bleu += bleu_sc
        count += 1

    return float(total_bleu / count) if count > 0 else 0.0


def compute_meteor(real_caps: list[str], syn_caps: list[str]) -> float:
    if len(real_caps) != len(syn_caps):
        raise ValueError(f"mismatch length real: {len(real_caps)} vs syn: {len(syn_caps)}")

    total_score = 0.0
    count = 0

    for ref_sentence, hyp_sentence in zip(real_caps, syn_caps):
        ref_tokens = ref_sentence.strip().split()
        hyp_tokens = hyp_sentence.strip().split()

        if len(ref_tokens) == 0 or len(hyp_tokens) == 0:
            continue

        score = meteor_score(
            [ref_tokens],
            hyp_tokens
        )

        total_score += score
        count += 1

    return float(total_score / count) if count > 0 else 0.0


def compute_rouge_l(reference_caps, generated_caps):
    total_f1 = 0.0
    count = 0
    for ref, hyp in zip(reference_caps, generated_caps):
        if len(ref.strip()) == 0 or len(hyp.strip()) == 0:
            continue
        sc = rouge.score(ref, hyp)
        total_f1 += sc["rougeL"].fmeasure
        count += 1
    return float(total_f1 / count) if count > 0 else 0.0


def compute_bert_score(reference_caps, generated_caps):
    P, R, F1 = bert_score(
        generated_caps,  # candidates
        reference_caps,  # references
        lang="en",
        rescale_with_baseline=True
    )
    return float(F1.mean().item())


def compute_clip_score(reference_caps, generated_caps):
    if not do_clipscore:
        return None

    all_sims = []
    clip_model.eval()

    for ref, hyp in zip(reference_caps, generated_caps):
        if len(ref.strip()) == 0 or len(hyp.strip()) == 0:
            continue

        inputs_ref = clip_processor(
            text=real_caps,
            padding=True,
            truncation=True,
            max_length=77,
            return_tensors="pt"
        ).to(clip_device)

        inputs_hyp = clip_processor(
            text=syn_caps,
            padding=True,
            truncation=True,
            max_length=77,
            return_tensors="pt"
        ).to(clip_device)

        with torch.no_grad():
            emb_ref = clip_model.get_text_features(**inputs_ref)
            emb_hyp = clip_model.get_text_features(**inputs_hyp)

        emb_ref_norm = emb_ref / emb_ref.norm(p=2, dim=-1, keepdim=True)
        emb_hyp_norm = emb_hyp / emb_hyp.norm(p=2, dim=-1, keepdim=True)

        pairwise_diag = torch.sum(emb_ref_norm * emb_hyp_norm, dim=-1)

        avg_cosine = pairwise_diag.mean().item()

    return avg_cosine


def compute_defect_keyword_f1(reference_caps, generated_caps):
    f1_list = []
    for ref, hyp in zip(reference_caps, generated_caps):
        ref_kw = set([kw for kw in defect_keywords if kw in ref.split()])
        hyp_kw = set([kw for kw in defect_keywords if kw in hyp.split()])

        if len(ref_kw) == 0 and len(hyp_kw) == 0:
            f1_list.append(1.0)
            continue
        if len(ref_kw) == 0 or len(hyp_kw) == 0:
            f1_list.append(0.0)
            continue

        intersection = ref_kw.intersection(hyp_kw)
        prec = len(intersection) / len(hyp_kw)
        rec = len(intersection) / len(ref_kw)
        f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) > 0 else 0.0
        f1_list.append(f1)

    return float(np.mean(f1_list)) if len(f1_list) > 0 else 0.0


summary_results = []
pairwise_metrics_all = []

# load all captions
caption_data = defaultdict(dict)
for system, base_path in base_dirs.items():
    for sub in subfolders:
        folder = os.path.join(base_path, sub)
        if os.path.isdir(folder):
            caption_data[system][sub] = load_captions(folder)
        else:
            caption_data[system][sub] = {}

# compute avg length, avg defect keywords, num samples
for system in base_dirs.keys():
    for sub in subfolders:
        caps_dict = caption_data[system].get(sub, {})
        all_caps = list(caps_dict.values())
        if len(all_caps) == 0:
            avg_len = 0.0
            avg_kw = 0.0
            n = 0
        else:
            avg_len = float(np.mean([len(c.split()) for c in all_caps]))
            avg_kw = float(np.mean([keyword_score(c) for c in all_caps]))
            n = len(all_caps)

        summary_results.append({
            "system": system,
            "domain": sub,
            "avg_caption_length": round(avg_len, 2),
            "avg_defect_keywords": round(avg_kw, 2),
            "num_samples": n
        })

# real to syn metric
for system in base_dirs.keys():
    for cls in ["io", "nio"]:
        real_key = f"real_{cls}"
        syn_key = f"syn_{cls}"
        real_dict = caption_data[system].get(real_key, {})
        syn_dict = caption_data[system].get(syn_key, {})

        common_fnames = sorted(set(real_dict.keys()) & set(syn_dict.keys()))
        if len(common_fnames) == 0:
            continue

        real_caps = [real_dict[f] for f in common_fnames]
        syn_caps = [syn_dict[f] for f in common_fnames]

        tfidf_sim = round(get_tfidf_similarity(real_caps, syn_caps), 3)
        sts_sim = round(get_sts_similarity(real_caps, syn_caps), 3)
        bleu_4 = round(compute_bleu(real_caps, syn_caps), 3)
        rouge_l_f1 = round(compute_rouge_l(real_caps, syn_caps), 3)
        meteor = round(compute_meteor(real_caps, syn_caps), 3)
        bert_f1 = round(compute_bert_score(real_caps, syn_caps), 3)
        defect_f1 = round(compute_defect_keyword_f1(real_caps, syn_caps), 3)

        if do_clipscore:
            clip_sim = round(compute_clip_score(real_caps, syn_caps), 3)
        else:
            clip_sim = None

        pairwise_metrics_all.append({
            "system": system,
            "type": cls,
            "aligned_samples": len(common_fnames),
            "TFIDF_sim": tfidf_sim,
            "STS_sim": sts_sim,
            "BLEU_4": bleu_4,
            "ROUGE_L_f1": rouge_l_f1,
            "METEOR": meteor,
            "BERTScore_F1": bert_f1,
            "CLIPScore": clip_sim,
            "DefectKeyword_F1": defect_f1
        })

with open(summary_csv, "w", newline="") as f:
    fields = ["system", "domain", "avg_caption_length", "avg_defect_keywords", "num_samples"]
    writer = csv.DictWriter(f, fieldnames=fields)
    writer.writeheader()
    writer.writerows(summary_results)

with open(similarity_csv, "w", newline="") as f:
    fields = list(pairwise_metrics_all[0].keys())
    writer = csv.DictWriter(f, fieldnames=fields)
    writer.writeheader()
    writer.writerows(pairwise_metrics_all)

print(f"Saved summary to {summary_csv}")
print(f"Saved similarity to {similarity_csv}")
